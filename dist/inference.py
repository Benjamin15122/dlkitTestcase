import tensorflow as tf
from mnist import IMAGE_PIXELS
import math

HIDDEN_UNITS = 100
LEARNING_RATE = 0.01

def predict(data):
    # Define the model
     # Variables of the hidden layer
    hid_w = tf.Variable(
        tf.truncated_normal(
            [IMAGE_PIXELS * IMAGE_PIXELS, HIDDEN_UNITS],
            stddev=1.0 / IMAGE_PIXELS),
        name="hid_w")
    hid_b = tf.Variable(tf.zeros([HIDDEN_UNITS]), name="hid_b")

    # Variables of the softmax layer
    sm_w = tf.Variable(
        tf.truncated_normal(
            [HIDDEN_UNITS, 10],
            stddev=1.0 / math.sqrt(100)),
        name="sm_w")
    sm_b = tf.Variable(tf.zeros([10]), name="sm_b")

    # Ops: located on the worker specified with FLAGS.task_index
    x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])
    y_ = tf.placeholder(tf.float32, [None, 10])

    hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)
    hid = tf.nn.relu(hid_lin)

    y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))
    cross_entropy = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))

    opt = tf.train.AdamOptimizer(LEARNING_RATE)

    keep_prob = tf.placeholder(tf.float32)

    init_op = tf.global_variables_initializer()
    saver = tf.train.Saver()

    with tf.Session() as sess:
        sess.run(init_op)
        saver.restore(sess, "/workspace/model/mnist.ckpt")
        #print ("Model restored.")
       
        prediction=tf.argmax(y,1)
        return prediction.eval(feed_dict={x: [data],keep_prob: 1.0}, session=sess)

def main():
    data = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15294117647058825, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.15294117647058825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09019607843137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09019607843137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.01568627450980392, 0.00392156862745098, 0.00784313725490196, 0.00784313725490196, 0.00392156862745098, 0.00392156862745098, 0.00392156862745098, 0.011764705882352941, 0.0196078431372549, 0.011764705882352941, 0.0, 0.0, 0.00392156862745098, 0.00784313725490196, 0.00392156862745098, 0.01568627450980392, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00392156862745098, 0.0, 0.0, 0.0, 0.0, 0.011764705882352941, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3607843137254902, 0.7529411764705882, 1.0, 0.7450980392156863, 0.0, 0.0, 0.00392156862745098, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43529411764705883, 1.0, 1.0, 1.0, 0.9137254901960784, 1.0, 0.11764705882352941, 0.0, 0.011764705882352941, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.00784313725490196, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.1803921568627451, 0.0, 0.011764705882352941, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.023529411764705882, 0.0, 0.33725490196078434, 1.0, 1.0, 0.7647058823529411, 0.023529411764705882, 0.0, 0.396078431372549, 1.0, 0.1411764705882353, 0.0, 0.011764705882352941, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.00392156862745098, 0.0, 0.0, 0.17254901960784313, 0.0, 0.0, 0.0, 0.0, 0.5882352941176471, 1.0, 0.2196078431372549, 0.0, 0.011764705882352941, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21176470588235294, 0.3764705882352941, 0.18823529411764706, 0.9490196078431372, 1.0, 0.0, 0.0, 0.011764705882352941, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.0, 0.0, 0.0, 0.25098039215686274, 0.8901960784313725, 1.0, 1.0, 1.0, 1.0, 0.41568627450980394, 0.0, 0.0, 0.011764705882352941, 0.011764705882352941, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00392156862745098, 0.0, 0.0, 0.0, 0.6313725490196078, 1.0, 0.7176470588235294, 0.0, 0.1843137254901961, 1.0, 1.0, 1.0, 0.19607843137254902, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.00784313725490196, 0.0, 0.0, 0.9215686274509803, 1.0, 0.47843137254901963, 0.0, 0.0, 1.0, 1.0, 0.30196078431372547, 0.9411764705882353, 1.0, 0.7607843137254902, 0.4745098039215686, 0.32941176470588235, 0.0, 0.09803921568627451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.7764705882352941, 1.0, 0.0, 0.0, 0.5843137254901961, 1.0, 0.8941176470588236, 0.0, 0.0, 0.0, 0.2196078431372549, 1.0, 1.0, 0.42745098039215684, 0.0, 0.10196078431372549, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.0, 0.24705882352941178, 1.0, 1.0, 0.9137254901960784, 1.0, 1.0, 0.615686274509804, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.0, 0.03137254901960784, 0.8784313725490196, 1.0, 0.8980392156862745, 0.3843137254901961, 0.0, 0.0, 0.0, 0.011764705882352941, 0.0, 0.00392156862745098, 0.00784313725490196, 0.0, 0.0, 0.00784313725490196, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098, 0.00392156862745098, 0.00392156862745098, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09411764705882353, 0.0, 0.011764705882352941, 0.011764705882352941, 0.0, 0.0, 0.0, 0.03137254901960784, 0.011764705882352941, 0.00392156862745098, 0.00392156862745098, 0.00784313725490196, 0.00784313725490196, 0.00784313725490196, 0.00784313725490196, 0.00392156862745098, 0.00392156862745098, 0.011764705882352941, 0.0, 0.09411764705882353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09019607843137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09019607843137255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15294117647058825, 0.10196078431372549, 0.10588235294117647, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10196078431372549, 0.10588235294117647, 0.10196078431372549, 0.15294117647058825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
    pred = predict(data)
    print(pred)


if __name__ == '__main__':
    main()